---
layout: single
title:  "오차 역전파 계산하기" #파이썬 스터디 3주차(실용 자연어처리 4주차)
categories:
   - DeepLearning
---

이전 포스팅[(보러가기)](https://tiny-bean.github.io/deeplearning/pystudyW3_1/)에서 오차 역전파의 개념과 기타 배경지식을 다뤘다.

이제 다층 퍼셉트론의 오차 역전파를 직접 계산해보자!

그런데 역전파 이전에 순전파가 선행되어야 하기 때문에, 순전파를 먼저 계산해야 한다.

-------
# 다층 퍼셉트론 모델의 순전파 계산

계산 과정은 필자의 손글씨로 설명하겠다 ... (왠지 부끄럽다)

계산에 사용할 다층 퍼셉트론 모델은 이런 구조를 가지고 있다.

weight와 bias, expected outputs도 임의로 설정했다.

![](/assets/images/2024-04-09-21-50-11.png)

--------

각 노드 안에서 일어나는 일을 좀 더 세분화하면 다음과 같다.

노드 내부에서는 입력값을 이용해 가중합(sum)을 만드는 단계와 이 가중합을 활성화 함수를 적용해 출력하는 단계로 구분된다.

![](/assets/images/2024-04-09-21-50-53.png)

-------

그럼, 이제 빨간 박스로 표시한 부분에서 일어나는 일들을 살펴보자.

h1노드 안에서,우선 입력값과 가중치의 곱들을 더하고 바이어스를 더해 sum을 구한다.

이후 sum에 활성화 함수를 적용해 output_h1을 출력한다.

앞으로의 예제에서는 활성화 함수로 시그모이드 함수를 사용하겠다.

![](/assets/images/2024-04-09-21-51-22.png)

-------

자, 이제 빨간 박스로 표시한 영역을 output layer 쪽으로 조금 이동했다.

여기에서 일어나는 일들을 알아보자!

![](/assets/images/2024-04-09-21-51-48.png)

이전 단계와 마찬가지로 o1과 o2 노드 안의 sum값과 output값을 각각 계산한다.




---

이제 출력층의 결과를 구했으니 etpected outputs 즉, 모델이 맞춰야 할 정답값과 결과 간의 오차율을 계산한다.

오차 계산에는 이전 포스팅에서 다뤘던 MSE loss를 이용한다.

![](/assets/images/2024-04-09-21-53-12.png)


E1 과 E2를 더해 E_total 값을 구해보니 0.24908 이 나왔다.

------

출력층까지 도달했고, 오차도 계산했으니 본격적으로 역전파를 계산해보자!

손실함수가 0이 되는 것을 목표로, 우리는 각각의 가중치를 수정해야한다.

따라서 각각의 가중치가 E_toral에 미치는 영향을 계산한다.

W5가 E_total에 미치는 영향을 알아보기 위해서 E_total을 W5에 대해서 편미분한다.

이 계산은 합성 함수 미분 공식을 따른다.

즉, 체인 룰(chain rule)에 따라 이렇게 계산할 수 있다.

![](/assets/images/2024-04-09-21-53-48.png)

작은 동그라미로 표시한 번호 순서대로 모두 편미분해준다.


![](/assets/images/2024-04-09-21-54-17.png) 

![](/assets/images/2024-04-09-21-54-43.png) 

![](/assets/images/2024-04-09-21-55-20.png) 

이렇게 오차 역전파 계산을 통해 W5값을 수정했다!


---

나머지 가중치도 같은 방법으로 계산해 업데이트 할 수 있다.

업데이트 된 가중치는 다음 순전파 계산에 반영된다.

------

사실 케라스나 텐서플로 같은 딥러닝 라이브러리를 활용하면 오늘 다룬 복잡한 계산을 직접 할 필요 없이 프로젝트를 수행할 수 있다.

하지만 딥러닝과 신경망을 깊이 이해하고 더 나은 결과를 도출하려면 어떤 알고리즘으로 작동하는지 알 필요가 있다.

아주 복잡하고 큰 계산기와도 같은 딥러닝 모델을 잘 다루고 싶다면, 

그 안에서 어떤 일들이 일어나는지 조금은 이해하고 넘어가야겠다!

드넓은 NLP의 세계에서 아직은 채워야 할 부분이 가득하지만

하나씩 필요한 것부터 쌓아가보려 한다.

아자잣 !!!

그럼 다음 포스팅으로 만나요. 안녕 😉

---------

> #### 참고자료

<모두의 딥러닝> 조태호







